from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List
from pydantic import BaseModel
import json
import os
from groq import Groq  # type: ignore # Import Groq client for LLaMa3 API integration

app = FastAPI()

# Enable CORS for all origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins, replace with specific domains for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Path for JSON data
ISSUES_FILE_PATH = "network_issues.json"

# Load issues from JSON
def load_issues() -> List[dict]:
    if os.path.exists(ISSUES_FILE_PATH):
        with open(ISSUES_FILE_PATH, "r") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                return []  # Return an empty list if JSON is invalid
    return []

# Save issues to JSON
def save_issues(issues: List[dict]):
    with open(ISSUES_FILE_PATH, "w") as f:
        json.dump(issues, f, indent=4)

# Define the data model for issues
class Issue(BaseModel):
    id: int
    title: str
    problem: str
    cause: str
    solution: str
    keywords: List[str]

# Utility function to format issues for output
def format_issue(issue: dict) -> str:
    return (
        f"**Issue ID**: {issue['id']}\n"
        f"**Title**: {issue['title']}\n"
        f"**Problem**: {issue['problem']}\n"
        f"**Cause**: {issue['cause']}\n"
        f"**Solution**: {issue['solution']}\n"
        f"**Keywords**: {', '.join(issue['keywords'])}\n"
        "----------------------------------------"
    )

# Initialize Groq client with API key for LLaMa3
client = Groq(api_key="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")  # Replace with actual API key

# Fetch response from LLaMa3 using Groq client
def get_llama_response(prompt: str) -> str:
    try:
        # Call the Groq API using the chat completions endpoint
        completion = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            temperature=1,
            max_tokens=1024,
            top_p=1,
            stream=True,
            stop=None,
        )
        
        response_text = ""
        for chunk in completion:
            response_text += chunk.choices[0].delta.content or ""
        
        return response_text

    except Exception as e:
        return f"Error contacting LLaMa3: {str(e)}"

# Create a new issue using LLaMa3 if none is found
def create_issue_from_llama(query: str, issues: List[dict]) -> dict:
    prompt = f"Could you help me understand the issue related to: {query}?"
    llama_response = get_llama_response(prompt)
    
    # Generate a new issue ID based on existing issues
    new_issue_id = max([issue["id"] for issue in issues], default=0) + 1
    new_issue = {
        "id": new_issue_id,
        "title": f"Generated Issue for '{query}'",
        "problem": query,
        "cause": "Generated by LLaMa3",
        "solution": llama_response,
        "keywords": [keyword.strip() for keyword in query.split()]
    }

    # Add the new issue to the list and save it
    issues.append(new_issue)
    save_issues(issues)
    return new_issue

# Root endpoint
@app.get("/")
def read_root():
    return {"message": "Welcome to the Network Issues Chatbot API with LLaMa3 integration"}

# Search issues by query
# Search issues by query
def search_issues(query: str, issues: List[dict]) -> List[dict]:
    query_lower = query.lower()
    return [
        issue for issue in issues
        if (issue.get('title') and query_lower in issue['title'].lower()) or 
           (issue.get('problem') and query_lower in issue['problem'].lower())
    ]


# Endpoint to get issues by query or use LLaMa3 if not found
@app.get("/issues")
def get_issue(query: Optional[str] = Query(None, description="Query for searching issues")):
    if not query:
        raise HTTPException(status_code=400, detail="Query parameter is required")
    
    issues = load_issues()
    results = search_issues(query, issues)
    
    if results:
        formatted_issues = "\n\n".join([format_issue(issue) for issue in results])
        return {"issues_found": formatted_issues}
    
    # No matching issues; create a new one using LLaMa3
    new_issue = create_issue_from_llama(query, issues)
    formatted_new_issue = format_issue(new_issue)
    return {
        "message": "No existing issue found. Created a new issue using LLaMa3:",
        "structured_issue": formatted_new_issue
    }
